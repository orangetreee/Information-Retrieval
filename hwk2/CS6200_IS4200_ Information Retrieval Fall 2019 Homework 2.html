<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0059)http://www.ccs.neu.edu/home/dasmith/courses/cs6200/hw2.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CS6200/IS4200: Information Retrieval Fall 2019 Homework 2</title>
</head>

<body>

<h1>CS6200/IS4200: Information Retrieval</h1>
<h2>Homework 2</h2>

<p>Return to <a href="http://www.ccs.neu.edu/home/dasmith/courses/cs6200/index.html">basic course information</a>.</p>

<p><strong>Assigned:</strong> Friday, 27 September 2019<br>
  <strong>Due:</strong> Friday, 11 October 2019, 11:59 p.m.</p>

<hr>

<h3>PageRank</h3>

<p>In this assignment, you will compute PageRank on a collection of
469,235 web sites.</p>

<p>Consider the version of PageRank described in class.  PageRank can be
computed iteratively as shown in the following pseudocode:

</p><pre>// P is the set of all pages; |P| = N
// S is the set of sink nodes, i.e., pages that have no out links
// M(p) is the set (without duplicates) of pages that link to page p
// L(q) is the number of out-links (without duplicates) from page q
// d is the PageRank damping/teleportation factor; use d = 0.85 as a fairly typical value

foreach page p in P
  PR(p) = 1/N                          /* initial value */

while PageRank has not converged do
  sinkPR = 0
  foreach page p in S                  /* calculate total sink PR */
    sinkPR += PR(p)
  foreach page p in P
    newPR(p) = (1-d)/N                 /* teleportation */
    newPR(p) += d*sinkPR/N             /* spread remaining sink PR evenly */
    foreach page q in M(p)             /* pages pointing to p */
      newPR(p) += d*PR(q)/L(q)         /* add share of PageRank from in-links */
  foreach page p
    PR(p) = newPR(p)

return PR
</pre>

<p>In order to facilitate the computation of PageRank using the above pseudocode, we would normally preprocess a document collection into a link graph, i.e., a set of records recording when document <code>p</code> links to document <code>q</code>.</p>

<p>Consider the following directed graph:</p>

<img alt="graph" src="./CS6200_IS4200_ Information Retrieval Fall 2019 Homework 2_files/pagerank.jpg">

<p>We can represent this graph as a collection of nodes, here, ordered pairs of node index and node name:

</p><pre>  0 A
  1 B
  2 C
  3 D
  4 E
  5 F
</pre>

and a collection of directed links, i.e., ordered pairs from source to target:

<pre>  0 1
  0 2
  0 5
  1 2
  1 3
  1 4
  1 5
  2 3
  2 4
  3 0
  3 2
  3 4
  3 5
  4 0
  5 0
  5 1
  5 4
</pre>

We use integer identifiers for the nodes for efficiency. Note that, unlike this example, in a real web graph, not every page will have in-links, nor will every page have out-links.<p></p>

<h3>Instructions</h3>

<ul>

<li>[10 points] Implement the iterative PageRank algorithm as described above.
    Test your code on the six-node example using the input
    representation given above.  Be sure that your code handles pages
    that have no in-links or out-links properly.  (You may wish to
    test on a few such examples.) In later parts of this assignment,
    your task will be easier if you don't require loading the entire
    link graph into memory.
  <p><em>Please hand in:</em> a list of the PageRank values you obtain for each of the six vertices after 1, 10, and 100 iterations of the PageRank algorithm.  You should have three values on each line: node id, node name, and PageRank value.</p></li>

<li>[20 points] Download <a href="http://www.ccs.neu.edu/home/dasmith/courses/cs6200/vertices-edu.txt.gz">this list of <code>.edu</code> web sites</a> and <a href="http://www.ccs.neu.edu/home/dasmith/courses/cs6200/edges-edu.txt.gz">this list of links among them</a> derived from the <a href="https://commoncrawl.org/2017/05/hostgraph-2017-feb-mar-apr-crawls/">Common Crawl</a> open-source web crawl.  For the sake of brevity, the data record links among websites, not web pages.  The formats for node and link data are the same as the toy example above.


    <p>
    Run your iterative version of PageRank algorithm until your PageRank
    values "converge".  To test for convergence, calculate the
    <a href="http://en.wikipedia.org/wiki/Perplexity">perplexity</a>
    of the PageRank distribution, where perplexity is simply 2 raised
    to the (Shannon)
    <a href="http://en.wikipedia.org/wiki/Entropy_(Information_theory)">entropy</a>
    of the PageRank distribution, i.e., 2<sup><small><em>H(PR)</em></small></sup>.
    Perplexity is a measure of how "skewed" a distribution is: the more "skewed"
    (i.e., less uniform) a distribution is, the lower its preplexity.  Informally,
    you can think of perplexity as measuring the number of elements that have a
    "reasonably large" probability weight; technically, the perplexity of a distribution
    with entropy <em>h</em> is the number of elements <em>n</em> such that a uniform
    distribution over <em>n</em> elements would also have entropy <em>h</em>.  (Hence,
    both distributions would be equally "unpredictable".)</p>

  <p>Run your iterative PageRank algorthm, outputting the perplexity of
    your PageRank distibution until the change in perplexity is less
    than 1 for at least <em>four</em> consecutive iterations.</p>

  <p>One hint is that in this dataset, some documents with
    high in-link count and high PageRank are the same, so
    don't worry that it's a bug.</p>

  <p><em>Please hand in:</em> a list of the perplexity values you obtain in
    each round until convergence as described above.</p>

</li><li>[20 points] Sort the collection of web pages by the PageRank values you obtain.
    <p>
      <em>Please hand in:</em> </p><ul>
	<li>a list of the document ID, website domain name, and PageRank of the
	  top <strong>50</strong> websites as sorted by PageRank;</li>
	<li>a list of the document ID, website domain name, and in-link count of the
	  top <strong>50</strong> websites by in-link count;</li>
	<li>the proportion of websites with no in-links (sources);</li>
	<li>the proportion of websites with no out-links (sinks); and</li>
	<li>the proportion of websites whose PageRank is <strong>less
	    than</strong> their initial, uniform values.</li>
      </ul>
</li>

</ul>

<h3>Final requests</h3>

<p>In addition to the written items mentioned above, you should hand
  in a copy of your source code, which should hopefully be relatively
  short, and instructions on (compiling and) running it.  The only
  input to your code should be the files in the vertex and edge
  formats described above.  The output should be a list of node IDs,
  website names, and their PageRank values.</p>




</body></html>